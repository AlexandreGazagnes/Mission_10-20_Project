#link to dataset
#https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis

# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-sVRsURrYZCH_XHSrYpIjrEdEO32gup
"""

!pip install pandas matplotlib tensorflow

import pandas as pd
df = pd.read_csv("/content/file.csv", encoding='utf-8', sep=',')

for col in df.columns:
    print(col)

df.columns = df.columns.str.replace(' ', '')

print(df)

review_df = df[['tweets', 'labels']]

print(review_df.shape)
review_df.head(5)

review_df = review_df[review_df['labels'] != 'neutral']

print(review_df.shape)
review_df.head(5)

review_df["labels"].value_counts()

sentiment_label = review_df.labels.factorize()
sentiment_label

tweet = review_df.tweets.values

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=5000)

tokenizer.fit_on_texts(tweet)

encoded_docs = tokenizer.texts_to_sequences(tweet)

from tensorflow.keras.preprocessing.sequence import pad_sequences

padded_sequence = pad_sequences(encoded_docs, maxlen=200)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding

vocab_size = ... #issue for vocab_size
embedding_vector_length = 32
model = Sequential()
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))
model.add(SpatialDropout1D(0.25))
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)
